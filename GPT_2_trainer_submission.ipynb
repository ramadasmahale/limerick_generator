{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install all dependencies"
      ],
      "metadata": {
        "id": "tApEy3o8_f86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cID5CNKZOZdC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MmbUubQxbrJ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!git clone https://github.com/unitaryai/detoxify\n",
        "!pip install transformers==4.16.2\n",
        "!pip install bitsandbytes-cuda111\n",
        "!git clone https://github.com/robgon-art/GRUEN\n",
        "!pip install wmd\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown --id 1S-l0L_YOzn5KhYHdB8iS37qKwuUhHP0G\n",
        "!gdown --id 10LpkO5Vm_zOu723FVk6cCeRsv_qyYLdL\n",
        "!unzip cola_model.zip\n",
        "!pip install phonemizer\n",
        "!sudo apt-get install festival\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env Variables"
      ],
      "metadata": {
        "id": "9iSkFgc5C82q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please change these paths appropriately before running the notebook\n",
        "POSTPROCESSING_DIR = '/content/drive/MyDrive/true_poetry/'\n",
        "GRUEN_DIR = \"GRUEN\"\n",
        "LIMERICK_DATA_PATH = '/content/drive/MyDrive/true_poetry/limerick_dataset.csv'"
      ],
      "metadata": {
        "id": "JD31Ub-fAmL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import all necessary libs"
      ],
      "metadata": {
        "id": "VLqLr0Ju_xrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9e1nyUZvyas"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import json\n",
        "import sys\n",
        "import copy\n",
        "import sys\n",
        "\n",
        "# Add imports of source code synced grom git\n",
        "# Gruen source code taken from -  https://github.com/robgon-art/GRUEN\n",
        "sys.path.append(GRUEN_DIR)\n",
        "import GRUEN.Main as gruen\n",
        "# Postprocessing library synced from the source - https://github.com/summerstay/true_poetry\n",
        "sys.path.append(POSTPROCESSING_DIR)\n",
        "from limerick_generator import init_limerick_generator, generate_limerick"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some of the libraries we use end up showing warnings from within\n",
        "# The below lines of code supress these warnings and make the output of the training more readable\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "lhuiocvCABNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data import"
      ],
      "metadata": {
        "id": "gb1LZnzGDOYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define names of columns of dataset\n",
        "IS_LIMERICK = 'is_limerick'\n",
        "AUTHOR = 'author'\n",
        "ID = 'id'\n",
        "LIMERICK = 'limerick'"
      ],
      "metadata": {
        "id": "2QbrNHbeDpxL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5g5yAd2yGm1"
      },
      "outputs": [],
      "source": [
        "limericks = pd.read_csv(LIMERICK_DATA_PATH)\n",
        "limericks = limericks[limericks[] == True]\n",
        "df = limericks\n",
        "df = df.drop(columns=[AUTHOR, ID, IS_LIMERICK])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yh6cARb9y1Z"
      },
      "outputs": [],
      "source": [
        "class Limericks(Dataset):\n",
        "    \n",
        "    def __init__(self, control_code, gpt2_type=\"gpt2\", max_length=1024):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.limericks = []\n",
        "        self.limericks_text = []\n",
        "\n",
        "        for row in df[LIMERICK]:\n",
        "          self.limericks.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"{row[:max_length]}<|endoftext|>\")\n",
        "            ))\n",
        "          row = row.replace('\\r\\n', ' ')\n",
        "          self.limericks_text.append(row)\n",
        "        self.limerick_count = len(self.limericks)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.limerick_count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.limericks[item], self.limericks_text[item]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDXY92xu-Wls"
      },
      "outputs": [],
      "source": [
        "dataset = Limericks(df[LIMERICK], gpt2_type=\"gpt2\")\n",
        "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7Szntz_cEvj3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo0ha0Dl_NhM"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter definitions\n",
        "num_epochs = 1\n",
        "lr = 2e-5\n",
        "batch_size = 1\n",
        "max_seq_len = 400\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=200, num_training_steps=-1)\n",
        "eps = 1e-5"
      ],
      "metadata": {
        "id": "vC5UtLN4F5Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function names\n",
        "GRUEN_LOSS = \"GRUEN_LOSS\"\n",
        "L2_LOSS = \"L2_LOSS\""
      ],
      "metadata": {
        "id": "hELZBIo1N2my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4odEAHB_l_J"
      },
      "outputs": [],
      "source": [
        "# Use this function to handle different batch sizes\n",
        "# Currently training has batch_size = 1 due to computational issues\n",
        "# TODO: Change later\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor\n",
        "    else:\n",
        "      raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XNxq3SY_lCc"
      },
      "outputs": [],
      "source": [
        "def train(dataset, model, tokenizer, batch_size=batch_size, epochs=num_epochs, lr=lr, max_seq_len=max_seq_len, save_model_on_epoch=True, custom_loss_fn = None):\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "    init_limerick_generator()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        batch_bar = tqdm(total=len(train_dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        total_loss = 0\n",
        "\n",
        "        for idx, (entry, text) in tqdm(enumerate(train_dataloader), position = 0, leave = True):\n",
        "            input_tensor = pack_tensor(entry, input_tensor, 768)\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = model(input_tensor, labels=input_tensor)\n",
        "\n",
        "            if not custom_loss_fn:\n",
        "              loss = outputs[0]\n",
        "            elif custom_loss_fn == GRUEN_LOSS:\n",
        "              greedy_op = torch.argmax(outputs[1][0, :, :], dim=1)\n",
        "              ips = tokenizer.decode(greedy_op)\n",
        "              gruen_score = gruen.get_gruen([ips])\n",
        "              loss = outputs[0]  / (torch.tensor(1 - gruen_score[0]) + eps)\n",
        "            elif  custom_loss_fn == L2_LOSS:\n",
        "              greedy_op = torch.argmax(outputs[1][0, :, :], dim=1)\n",
        "              ips = tokenizer.decode(greedy_op)\n",
        "              loss = torch.norm( ((greedy_op.float() - input_tensor[0].float()) ** 2) , p=2)\n",
        "          \n",
        "            total_loss += float(loss)\n",
        "            loss.backward()\n",
        "          \n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "            # tqdm lets you add some details so you can monitor training as you train.\n",
        "            batch_bar.set_postfix(loss=\"{:.04f}\".format(float(loss / (idx + 1)), lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))))\n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "            batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "        if save_model_on_epoch:\n",
        "           torch.save({\n",
        "              'epoch': epoch+1,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': total_loss/len(train_dataloader),\n",
        "              }, f'/content/drive/MyDrive/project_model.pt')\n",
        "\n",
        "        print(\"Epoch {}/{}: Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, total_loss/len(train_dataloader), lr))    \n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "naOsrpqW_1x8"
      },
      "outputs": [],
      "source": [
        "model = train(dataset, model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example poetry generation"
      ],
      "metadata": {
        "id": "gMvuBtrSLLJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pnSOTo8s_cE"
      },
      "outputs": [],
      "source": [
        "prompt = \"The broadcasts and newspapers pull\"\n",
        "model_cpy = copy.deepcopy(model)\n",
        "generate_limerick(prompt, model_cpy.to('cpu'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "GPT-2_trainer_submission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}